{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps= PorterStemmer()\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "import string\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "class preprocessing():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def removePunctuations(self,data):\n",
    "            temp=re.sub(r'[^\\w\\s%]',' ',data.lower())            \n",
    "            temp=re.sub(r'\\s+',' ',temp)\n",
    "            return temp\n",
    "    \n",
    "    def removeStopWords(self,data):\n",
    "            words=word_tokenize(data)  \n",
    "            words = [ps.stem(w) for w in words if not w in cachedStopWords]  \n",
    "            return \" \".join(words)\n",
    "        \n",
    "    def preprocess(self,text):\n",
    "        processed_text=[]\n",
    "        for i in text:\n",
    "            #Remove punctuations\n",
    "            #print(i)\n",
    "            i=self.removePunctuations(i)\n",
    "            tokens=self.removeStopWords(i)\n",
    "            processed_text.append(\"\".join(tokens))\n",
    "        #print(processed_text)\n",
    "        return processed_text\n",
    "    \n",
    "    def getSocialKeywords(self,text):\n",
    "        processed_text=[]\n",
    "        for sent in text:\n",
    "            doc=nlp(sent)\n",
    "            pos_text=\"\"\n",
    "            for token in doc:                 \n",
    "                if((str(token.pos_) is not \"VERB\") and (str(token.pos_) is not \"ADV\") and (str(token.pos_) is not \"ADP\") and (str(token.pos_) is not \"PUNCT\")):\n",
    "                    pos_text=pos_text+\" \"+str(token)\n",
    "            \n",
    "            processed_text.append(pos_text)\n",
    "            \n",
    "        #keywords=getSKeywords(processed_text)\n",
    "        return processed_text\n",
    "    \n",
    "    def getAgencyKeywords(self,text):\n",
    "            processed_text=[]\n",
    "            for sent in text:\n",
    "                doc=nlp(sent)\n",
    "                pos_text=\"\"\n",
    "                for token in doc: \n",
    "\n",
    "                    if((str(token.pos_) is not \"ADV\") and (str(token.pos_) is not \"ADP\")):\n",
    "                        pos_text=pos_text+\" \"+str(token)\n",
    "\n",
    "                processed_text.append(pos_text)\n",
    "\n",
    "            #keywords=getSKeywords(processed_text)\n",
    "            return processed_text\n",
    "        \n",
    "    def parse_Resumefile(self,data,n):\n",
    "\n",
    "        #This consists of ngram as key & count as value\n",
    "        words=data.split(\" \")\n",
    "\n",
    "        #Prepare n-grams\n",
    "        ngrams=self.ngrams(words,n)\n",
    "        \n",
    "        return list(ngrams)\n",
    "    \n",
    "    def build_ngram_distribution(self,data):\n",
    "        n_s=[1,2,3] #uni, bi, and trigrams\n",
    "        dist=[]\n",
    "        for n in n_s:\n",
    "            ngrams=self.parse_Resumefile(data,n)\n",
    "            for i in ngrams:\n",
    "                dist.append(\" \".join(i))\n",
    "        return dist\n",
    "    \n",
    "    \n",
    "    #extracts all possible n-grams from the words stiched together\n",
    "    def ngrams(self,input_list, n):\n",
    "        return (zip(*[input_list[i:] for i in range(n)]))\n",
    "\n",
    "    def getSocialNGrams(self,trainData):\n",
    "        ngrams=[]\n",
    "        trainData=self.preprocess(trainData)\n",
    "        for row in trainData:\n",
    "            ngrams.append(self.build_ngram_distribution(row))\n",
    "        return ngrams\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " [['happi', 'son', 'got', '90', '%', 'mark', 'examin', 'happi son', 'son got', 'got 90', '90 %', '% mark', 'mark examin', 'happi son got', 'son got 90', 'got 90 %', '90 % mark', '% mark examin'], ['went', 'movi', 'friend', 'fun', 'went movi', 'movi friend', 'friend fun', 'went movi friend', 'movi friend fun'], ['hot', 'kiss', 'girl', 'friend', 'last', 'night', 'made', 'day', 'hot kiss', 'kiss girl', 'girl friend', 'friend last', 'last night', 'night made', 'made day', 'hot kiss girl', 'kiss girl friend', 'girl friend last', 'friend last night', 'last night made', 'night made day'], ['son', 'woke', 'fantast', 'breakfast', 'egg', 'special', 'hamburg', 'patti', 'pancak', 'son woke', 'woke fantast', 'fantast breakfast', 'breakfast egg', 'egg special', 'special hamburg', 'hamburg patti', 'patti pancak', 'son woke fantast', 'woke fantast breakfast', 'fantast breakfast egg', 'breakfast egg special', 'egg special hamburg', 'special hamburg patti', 'hamburg patti pancak'], ['older', 'daughter', 'keep', 'pat', 'younger', 'daughter', 'head', 'older daughter', 'daughter keep', 'keep pat', 'pat younger', 'younger daughter', 'daughter head', 'older daughter keep', 'daughter keep pat', 'keep pat younger', 'pat younger daughter', 'younger daughter head']]\n"
     ]
    }
   ],
   "source": [
    "# # # In[41]:\n",
    "\n",
    "import pandas as pd\n",
    "train_data=pd.read_csv(\"trainData.csv\")\n",
    "s=train_data['moment'].head(5)\n",
    "#s=pd.\"But I am the real Strider, fortunately.I am Aragorn son of Arathorn; and if by life or death I can saved you, I will, I am real.\"\n",
    "def main():\n",
    "    p=preprocessing()\n",
    "    #print(p.preprocess(s))\n",
    "    #print(p.propernouns(s))\n",
    "    #print(p.getSocialKeywords(s),\"\\n\")\n",
    "    print(\"\\n\\n\\n\",p.getSocialNGrams(s))\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
