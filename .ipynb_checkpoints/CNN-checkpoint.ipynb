{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "aL5aA_1Sgkz5",
    "outputId": "6f69d87f-37b3-4d84-fd5a-0a661bb34cd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shreya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shreya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shreya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shreya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shreya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shreya/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glove import Corpus, Glove\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7MvHhKegyPU"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "P4HmITd9gk0u",
    "outputId": "d2dee502-5d86-4271-d022-77af20d29b6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10560, 12)\n",
      "    hmid                                             moment          concepts  \\\n",
      "0  27674  I was happy when my son got 90% marks in his e...  education|family   \n",
      "1  27685          went to movies with my friends it was fun     entertainment   \n",
      "2  27691  A hot kiss with my girl friend last night made...           romance   \n",
      "3  27701  My son woke me up to a fantastic breakfast of ...       family|food   \n",
      "4  27712  My older daughter keeps patting my younger dau...            family   \n",
      "\n",
      "  agency social   age country gender  married parenthood reflection  \\\n",
      "0     no    yes  29.0     IND      m  married          y        24h   \n",
      "1    yes    yes  29.0     IND      m   single          y        24h   \n",
      "2    yes    yes  25.0     IND      m  married          y        24h   \n",
      "3     no    yes    79     USA      f  widowed          y        24h   \n",
      "4     no    yes    30     USA      f  married          y        24h   \n",
      "\n",
      "                      duration  \n",
      "0                   half_a_day  \n",
      "1                   half_a_day  \n",
      "2            at_least_one_hour  \n",
      "3  all_day_im_still_feeling_it  \n",
      "4                 a_few_moment  \n"
     ]
    }
   ],
   "source": [
    "# read the dataset \n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aYZk3lk-gk03",
    "outputId": "cb6abbd7-aace-4ce3-dcd5-36324cf8386c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  moment agency social\n",
      "0      I was happy when my son got 90% marks in his e...     no    yes\n",
      "1              went to movies with my friends it was fun    yes    yes\n",
      "2      A hot kiss with my girl friend last night made...    yes    yes\n",
      "3      My son woke me up to a fantastic breakfast of ...     no    yes\n",
      "4      My older daughter keeps patting my younger dau...     no    yes\n",
      "5          I cooked my girlfriend a wonderful breakfast.    yes    yes\n",
      "6         My Mother gave me a surprise visit at my home.     no    yes\n",
      "7      There was hardly any traffic on my way to work...    yes     no\n",
      "8                     I came to my office at right time.    yes     no\n",
      "9      The day I got my degree in industrial engineering    yes     no\n",
      "10     I went to office hour of one of my professors,...    yes    yes\n",
      "11     We all ladies member from my family went for a...    yes    yes\n",
      "12     When my wife came home from work and we shared...    yes    yes\n",
      "13                        My father bought me a bicycle.     no    yes\n",
      "14     I went to my home and given an Ice Cream Famil...    yes    yes\n",
      "15     I was able to play my video game that I enjoy ...    yes     no\n",
      "16     I was able to do a full hour of research on a ...    yes     no\n",
      "17     When i got to the train station it just pulled...     no     no\n",
      "18     I went to support my cousin in her product lau...    yes    yes\n",
      "19     I cooked a new recipe for dinner and it turned...    yes     no\n",
      "20     The weather has been warm and gorgeous for the...     no     no\n",
      "21     Had dinner with my Family, now that my dad is ...    yes    yes\n",
      "22     A hot chat with my dating partner makes me fee...    yes    yes\n",
      "23     The rain stopped and beautiful sunlight appear...    yes     no\n",
      "24                          I had a very delicious lunch    yes     no\n",
      "25     I finally went for a run and worked out and it...    yes     no\n",
      "26     I received a call letter from the University f...     no     no\n",
      "27                     my hole family out to have dinner    yes    yes\n",
      "28     yesterday i received a message that i got prom...    yes    yes\n",
      "29                         A good win for my sports team     no    yes\n",
      "...                                                  ...    ...    ...\n",
      "10530                                 I WENT TO SHOPPING    yes     no\n",
      "10531  My uncle is working abroad, when he is here, I...     no    yes\n",
      "10532  Enjoying my new pair of sunnies, riding around...    yes     no\n",
      "10533  My uncle is working abroad, when he is here, I...     no    yes\n",
      "10534  MY UNCLE IS WORKING IN  ABROAD WHEN HE IS HERE...     no    yes\n",
      "10535  We buyed some cookies, sweets, clothes to our ...    yes    yes\n",
      "10536                farewell party to enjoy my friends.    yes    yes\n",
      "10537          My cousin made it through surgery safely.     no    yes\n",
      "10538  A new episode of my favorite podcasts came out...     no     no\n",
      "10539  One event that made me feel happy last night w...    yes    yes\n",
      "10540                 i went to my old school yesterday.    yes     no\n",
      "10541          I made big progress on a project at work.    yes     no\n",
      "10542     Spent all almost all day with my granddaughter    yes    yes\n",
      "10543             I enjoyed a home-cooked Southern meal.    yes     no\n",
      "10544  Listening to my favourite music in my car on t...    yes     no\n",
      "10545    I ate some delicious eggrolls that my mom made.    yes     no\n",
      "10546  I'm reading the 3rd novel of Jacqueline Carey'...    yes     no\n",
      "10547  I had a very nice meal with my spouse without ...    yes    yes\n",
      "10548  I read a new novel which was so interesting an...    yes     no\n",
      "10549  We all family members went to a nearby restaur...    yes    yes\n",
      "10550  I was praised by my boss for an idea I had at ...     no    yes\n",
      "10551  A happy event that made me happy in the past 2...    yes     no\n",
      "10552  I spent time with my wife watching TV last night.    yes    yes\n",
      "10553  I accepted financial aid for this upcoming sch...    yes     no\n",
      "10554  Yesterday after court I took an extra hour to ...    yes    yes\n",
      "10555  My husband called me just to tell me he loved me.     no    yes\n",
      "10556     I worked out, which always makes me feel good.    yes     no\n",
      "10557  Finally got to watch the new Resident Evil movie.    yes     no\n",
      "10558  I got to talk to an old friend and reminisce o...    yes    yes\n",
      "10559  I had a great meeting yesterday at work with m...    yes    yes\n",
      "\n",
      "[10560 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get Moment, agency and social\n",
    "target_data = data[['moment', 'agency', 'social']]\n",
    "print(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "0FVaV6lvgk0_",
    "outputId": "cc037aae-9f2a-470f-ac39-317451153fda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Social -------------------\n",
      "               YES        NO      SUM\n",
      "Agency YES     3554       4242    7796\n",
      "       NO      2071       693     2764\n",
      "       SUM     5625       4935    10560\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis\n",
    "global agency\n",
    "global social\n",
    "agency = list(target_data['agency'])\n",
    "moments = list(target_data['moment'])\n",
    "social = list(target_data['social'])\n",
    "yy = 0\n",
    "yn = 0\n",
    "nn = 0\n",
    "ny = 0\n",
    "# print(\"Len: \",len(moments))\n",
    "for i in range(len(moments)):\n",
    "    if agency[i] == 'yes' and social[i] =='yes':\n",
    "        yy += 1\n",
    "    elif agency[i] == 'yes' and social[i] == 'no':\n",
    "        yn += 1\n",
    "    elif agency[i] == 'no' and social[i] =='yes':\n",
    "        ny += 1\n",
    "    else:\n",
    "        nn += 1\n",
    "\n",
    "print(\"------------------ Social -------------------\")\n",
    "print(\"               YES        NO      SUM\")\n",
    "print(\"Agency YES    \",yy,\"     \",yn,\"  \",yy+yn)\n",
    "print(\"       NO     \",ny,\"     \",nn,\"   \",ny+nn)\n",
    "print(\"       SUM    \",yy+ny,\"     \",yn+nn,\"  \",yy+nn+ny+yn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fp4IanWJgk1I"
   },
   "outputs": [],
   "source": [
    "# Observation: Data is positive in high proportion although there is an imbalance as more positive data\n",
    "# for social than for agency. This might result in poor accuracy while predicting the label agency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGYZbWNmgk1Q"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import statistics\n",
    "def tokenizer(sentence):\n",
    "    tokenizer = RegexpTokenizer('[a-zA-Z0-9\\']+')\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqhdcP3Rgk1W"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def stopWordRemoval(words):\n",
    "    filtered_data = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for word in words:\n",
    "        if not word in stop_words:\n",
    "            if word.isnumeric() or word.isalpha():\n",
    "                filtered_data.append(word)\n",
    "    return filtered_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "YP1cFo3Ogk1b",
    "outputId": "9fea4423-3839-4fc6-e7bd-0d6ea482562b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in Vocabulary :  7604\n",
      "------------------ SENTENCE STATISTICS OF MOMENTS -----------------\n",
      "Minimum Length :  2\n",
      "Maximum Length :  70\n",
      "Average Length :  13\n",
      "Median Length  :  12\n"
     ]
    }
   ],
   "source": [
    "# Data Pre-processing\n",
    "global sentence_list\n",
    "global sequenced_vocab\n",
    "global sentences\n",
    "global vocab\n",
    "# 1. Split the sentences in moments into words (using regex tokenizer)\n",
    "vocab = list()\n",
    "sequenced_vocab = dict()\n",
    "index = 1\n",
    "sentence_list = list()\n",
    "seq_list = list()\n",
    "sentences = list()\n",
    "\n",
    "for i in range(len(moments)):\n",
    "    moments[i] = moments[i].lower()\n",
    "    bag_of_words = tokenizer(moments[i])\n",
    "    #remove stop words\n",
    "#     bag_of_words = stopWordRemoval(bag_of_words)\n",
    "    sentence_list.append(bag_of_words)\n",
    "    vocab += bag_of_words\n",
    "\n",
    "# sentences = sentence_list\n",
    "\n",
    "# Form sequence dictionary and convert each sentence into a number sequence \n",
    "for word in vocab:\n",
    "    if word not in sequenced_vocab.keys():\n",
    "        sequenced_vocab[word] = index\n",
    "        index += 1\n",
    "print(\"Unique words in Vocabulary : \",len(sequenced_vocab))\n",
    "# print(\"Sequenced Vocab : \")\n",
    "# print(sequenced_vocab)\n",
    "\n",
    "# Avg length of each sentence\n",
    "sentence_length = [ len(sentence_list[i]) for i in range(len(sentence_list))]\n",
    "sentence_length.sort()\n",
    "print(\"------------------ SENTENCE STATISTICS OF MOMENTS -----------------\")\n",
    "print(\"Minimum Length : \",sentence_length[0])\n",
    "print(\"Maximum Length : \",sentence_length[len(sentence_length)-1])\n",
    "print(\"Average Length : \",round(sum(sentence_length)/len(sentence_length)))\n",
    "print(\"Median Length  : \", round(statistics.median(sentence_length)))\n",
    "# print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "gdHRDphfvyic",
    "outputId": "64efcee2-d8aa-479c-da45-3dd9d666d5f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences :  10560\n",
      "Length of majority sentences:  8\n",
      " ================= PADDED SENTENCE =================\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   1 337  20 209 338  78   2\n",
      " 333 736]\n"
     ]
    }
   ],
   "source": [
    "# 2. Assign a sequence of numbers to each of the sentences \n",
    "global sequenced_vocab\n",
    "global sentence_list\n",
    "for i in range(len(sentence_list)):\n",
    "    for word in sentence_list[i]:\n",
    "        if word in sequenced_vocab.keys():\n",
    "            seq_list.append(sequenced_vocab[word])\n",
    "    sentence_list[i] = seq_list\n",
    "    seq_list = []\n",
    "    \n",
    "    \n",
    "#Printing Sentence Stats\n",
    "count_len = dict()\n",
    "for length in sentence_length:\n",
    "    if length not in count_len.keys():\n",
    "        count_len[length] = 1\n",
    "    else:\n",
    "        count_len[length] += 1\n",
    "\n",
    "print(\"Total Sentences : \",len(sentence_list))\n",
    "print(\"Length of majority sentences: \",max(count_len.items(), key=operator.itemgetter(1))[0])\n",
    "\n",
    "# # 3. Pad with 0s if sentence length is not same (at the beginning)\n",
    "padded_sentence = pad_sequences(sentence_list, maxlen=20)\n",
    "print(\" ================= PADDED SENTENCE =================\")\n",
    "print(padded_sentence[9999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjlJ_N4Dgk1i"
   },
   "outputs": [],
   "source": [
    "#transform agency and social to binary labels\n",
    "global agency_label\n",
    "global social_label\n",
    "agency_label = [ 0 if agency[i] == \"no\" else 1 for i in range(len(agency))]\n",
    "social_label = [ 0 if social[i] == \"no\" else 1 for i in range(len(social))]\n",
    "# print(agency_label)\n",
    "# print(social_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhjTDETjRqnp"
   },
   "outputs": [],
   "source": [
    "def createEmbMatrix(vectors, word_dict):\n",
    "  max_words = len(list(word_dict.values()))\n",
    "  embedding_matrix = np.zeros((max_words, 100))\n",
    "  for i in range(max_words):\n",
    "    embedding_vector = vectors[i]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0G_5A82rmRq"
   },
   "outputs": [],
   "source": [
    "def runCNNModel(sentences_train, y_train, sentences_val, y_val, sentences_test, y_test):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(7920,100,input_length=20))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "  model.add(layers.GlobalMaxPooling1D())\n",
    "  model.add(layers.Dense(256))\n",
    "  # model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Activation('relu'))\n",
    "  model.add(layers.Dense(1))\n",
    "  model.add(layers.Activation('sigmoid'))\n",
    "  model.summary()\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(np.array(sentences_train), np.array(y_train),\n",
    "                      epochs=10,\n",
    "                      batch_size=32,\n",
    "                      verbose=1,\n",
    "                      validation_data=(np.array(sentences_test), np.array(y_test)))\n",
    "  loss, accuracy = model.evaluate(np.array(sentences_test), np.array(y_test), verbose=1)\n",
    "  print(\"Loss: \", loss)\n",
    "  print(\"Accuracy : \",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloveCNNModel(sentences_train, y_train, sentences_val, y_val, sentences_test, y_test, embedding_matrix):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(7605,100,weights=[embedding_matrix],input_length=20,trainable=False))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
    "  model.add(layers.GlobalMaxPooling1D())\n",
    "  model.add(layers.Dense(256))\n",
    "#   model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Activation('relu'))\n",
    "  model.add(layers.Dense(1))\n",
    "  model.add(layers.Activation('sigmoid'))\n",
    "  model.summary()\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(np.array(sentences_train), np.array(y_train),\n",
    "                      epochs=10,\n",
    "                      batch_size=32,\n",
    "                      verbose=1,\n",
    "                      validation_data=(np.array(sentences_val), np.array(y_val)))\n",
    "  loss, accuracy = model.evaluate(np.array(sentences_test), np.array(y_test), verbose=1)\n",
    "  print(\"Loss: \", loss)\n",
    "  print(\"Accuracy : \",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uP3V4jAggk1n",
    "outputId": "ad920272-43a9-463b-f506-1a0d11f21f06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE:  7605\n",
      "Embedding Matrix :  (7605, 100)\n",
      "================================ For SOCIAL ==============================\n",
      "----------- TRAIN ---------------\n",
      "Shape:  6336\n",
      "----------- VALIDATE --------------\n",
      "Shape:  845\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 100)           792000    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 18, 64)            19264     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 828,161\n",
      "Trainable params: 828,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6336 samples, validate on 845 samples\n",
      "Epoch 1/10\n",
      "6336/6336 [==============================] - 5s 712us/step - loss: 0.4015 - acc: 0.8051 - val_loss: 0.2669 - val_acc: 0.8947\n",
      "Epoch 2/10\n",
      "6336/6336 [==============================] - 4s 673us/step - loss: 0.2006 - acc: 0.9249 - val_loss: 0.2442 - val_acc: 0.9041\n",
      "Epoch 3/10\n",
      "6336/6336 [==============================] - 4s 663us/step - loss: 0.1174 - acc: 0.9628 - val_loss: 0.2778 - val_acc: 0.9030\n",
      "Epoch 4/10\n",
      "6336/6336 [==============================] - 4s 664us/step - loss: 0.0759 - acc: 0.9784 - val_loss: 0.3129 - val_acc: 0.8888\n",
      "Epoch 5/10\n",
      "6336/6336 [==============================] - 4s 672us/step - loss: 0.0418 - acc: 0.9891 - val_loss: 0.4155 - val_acc: 0.8805\n",
      "Epoch 6/10\n",
      "6336/6336 [==============================] - 4s 675us/step - loss: 0.0272 - acc: 0.9924 - val_loss: 0.4411 - val_acc: 0.8840\n",
      "Epoch 7/10\n",
      "6336/6336 [==============================] - 4s 677us/step - loss: 0.0169 - acc: 0.9954 - val_loss: 0.4883 - val_acc: 0.8840\n",
      "Epoch 8/10\n",
      "6336/6336 [==============================] - 4s 678us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.5168 - val_acc: 0.8805\n",
      "Epoch 9/10\n",
      "6336/6336 [==============================] - 4s 681us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.5680 - val_acc: 0.8828\n",
      "Epoch 10/10\n",
      "6336/6336 [==============================] - 4s 681us/step - loss: 0.0058 - acc: 0.9989 - val_loss: 0.5975 - val_acc: 0.8899\n",
      "845/845 [==============================] - 0s 45us/step\n",
      "Loss:  0.5974671066600895\n",
      "Accuracy :  88.99408284023669\n",
      "Using Glove trained on Model ===> \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 100)           760500    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 18, 64)            19264     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 796,661\n",
      "Trainable params: 36,161\n",
      "Non-trainable params: 760,500\n",
      "_________________________________________________________________\n",
      "Train on 6336 samples, validate on 3379 samples\n",
      "Epoch 1/10\n",
      "6336/6336 [==============================] - 1s 198us/step - loss: 0.5447 - acc: 0.7289 - val_loss: 0.4919 - val_acc: 0.7576\n",
      "Epoch 2/10\n",
      "6336/6336 [==============================] - 1s 153us/step - loss: 0.4801 - acc: 0.7699 - val_loss: 0.4637 - val_acc: 0.7822\n",
      "Epoch 3/10\n",
      "6336/6336 [==============================] - 1s 161us/step - loss: 0.4475 - acc: 0.7953 - val_loss: 0.4595 - val_acc: 0.7825\n",
      "Epoch 4/10\n",
      "6336/6336 [==============================] - 1s 154us/step - loss: 0.4396 - acc: 0.8016 - val_loss: 0.4575 - val_acc: 0.7922\n",
      "Epoch 5/10\n",
      "6336/6336 [==============================] - 1s 158us/step - loss: 0.4187 - acc: 0.8082 - val_loss: 0.4259 - val_acc: 0.8014\n",
      "Epoch 6/10\n",
      "6336/6336 [==============================] - 1s 128us/step - loss: 0.4221 - acc: 0.8082 - val_loss: 0.4224 - val_acc: 0.8088\n",
      "Epoch 7/10\n",
      "6336/6336 [==============================] - 1s 122us/step - loss: 0.4108 - acc: 0.8172 - val_loss: 0.4295 - val_acc: 0.8020\n",
      "Epoch 8/10\n",
      "6336/6336 [==============================] - 1s 122us/step - loss: 0.4052 - acc: 0.8164 - val_loss: 0.4175 - val_acc: 0.8070\n",
      "Epoch 9/10\n",
      "6336/6336 [==============================] - 1s 121us/step - loss: 0.4034 - acc: 0.8201 - val_loss: 0.4100 - val_acc: 0.8168\n",
      "Epoch 10/10\n",
      "6336/6336 [==============================] - 1s 121us/step - loss: 0.4007 - acc: 0.8209 - val_loss: 0.4079 - val_acc: 0.8106\n",
      "845/845 [==============================] - 0s 34us/step\n",
      "Loss:  0.4096703724042904\n",
      "Accuracy :  80.59171600454658\n",
      "============================= For AGENCY =================================\n",
      "----------- TRAIN ---------------\n",
      "Shape:  6336\n",
      "----------- VALIDATE --------------\n",
      "Shape:  845\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 100)           792000    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 18, 64)            19264     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 828,161\n",
      "Trainable params: 828,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6336 samples, validate on 845 samples\n",
      "Epoch 1/10\n",
      "6336/6336 [==============================] - 4s 672us/step - loss: 0.4601 - acc: 0.7808 - val_loss: 0.3736 - val_acc: 0.8331\n",
      "Epoch 2/10\n",
      "6336/6336 [==============================] - 4s 610us/step - loss: 0.3092 - acc: 0.8677 - val_loss: 0.3549 - val_acc: 0.8414\n",
      "Epoch 3/10\n",
      "6336/6336 [==============================] - 4s 612us/step - loss: 0.2052 - acc: 0.9208 - val_loss: 0.3848 - val_acc: 0.8367\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6336/6336 [==============================] - 4s 577us/step - loss: 0.1229 - acc: 0.9547 - val_loss: 0.4496 - val_acc: 0.8343\n",
      "Epoch 5/10\n",
      "6336/6336 [==============================] - 4s 571us/step - loss: 0.0673 - acc: 0.9789 - val_loss: 0.5490 - val_acc: 0.8367\n",
      "Epoch 6/10\n",
      "6336/6336 [==============================] - 4s 573us/step - loss: 0.0390 - acc: 0.9871 - val_loss: 0.6307 - val_acc: 0.8320\n",
      "Epoch 7/10\n",
      "6336/6336 [==============================] - 4s 573us/step - loss: 0.0233 - acc: 0.9926 - val_loss: 0.6393 - val_acc: 0.8343\n",
      "Epoch 8/10\n",
      "6336/6336 [==============================] - 4s 572us/step - loss: 0.0161 - acc: 0.9957 - val_loss: 0.7505 - val_acc: 0.8308\n",
      "Epoch 9/10\n",
      "6336/6336 [==============================] - 4s 576us/step - loss: 0.0168 - acc: 0.9951 - val_loss: 0.7320 - val_acc: 0.8379\n",
      "Epoch 10/10\n",
      "6336/6336 [==============================] - 4s 572us/step - loss: 0.0120 - acc: 0.9972 - val_loss: 0.7804 - val_acc: 0.8308\n",
      "845/845 [==============================] - 0s 38us/step\n",
      "Loss:  0.7804342894159125\n",
      "Accuracy :  83.07692309808449\n",
      "Using Glove trained on Model ===> \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 20, 100)           760500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 18, 64)            19264     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 796,661\n",
      "Trainable params: 36,161\n",
      "Non-trainable params: 760,500\n",
      "_________________________________________________________________\n",
      "Train on 6336 samples, validate on 3379 samples\n",
      "Epoch 1/10\n",
      "6336/6336 [==============================] - 1s 177us/step - loss: 0.4882 - acc: 0.7751 - val_loss: 0.4663 - val_acc: 0.7982\n",
      "Epoch 2/10\n",
      "6336/6336 [==============================] - 1s 113us/step - loss: 0.4488 - acc: 0.8000 - val_loss: 0.4476 - val_acc: 0.8047\n",
      "Epoch 3/10\n",
      "6336/6336 [==============================] - 1s 116us/step - loss: 0.4388 - acc: 0.8021 - val_loss: 0.4402 - val_acc: 0.8038\n",
      "Epoch 4/10\n",
      "6336/6336 [==============================] - 1s 116us/step - loss: 0.4315 - acc: 0.8106 - val_loss: 0.4455 - val_acc: 0.8005\n",
      "Epoch 5/10\n",
      "6336/6336 [==============================] - 1s 113us/step - loss: 0.4239 - acc: 0.8122 - val_loss: 0.4284 - val_acc: 0.8130\n",
      "Epoch 6/10\n",
      "6336/6336 [==============================] - 1s 114us/step - loss: 0.4202 - acc: 0.8158 - val_loss: 0.4338 - val_acc: 0.8127\n",
      "Epoch 7/10\n",
      "6336/6336 [==============================] - 1s 113us/step - loss: 0.4201 - acc: 0.8155 - val_loss: 0.4286 - val_acc: 0.8112\n",
      "Epoch 8/10\n",
      "6336/6336 [==============================] - 1s 115us/step - loss: 0.4112 - acc: 0.8199 - val_loss: 0.4240 - val_acc: 0.8139\n",
      "Epoch 9/10\n",
      "6336/6336 [==============================] - 1s 115us/step - loss: 0.4106 - acc: 0.8213 - val_loss: 0.4202 - val_acc: 0.8215\n",
      "Epoch 10/10\n",
      "6336/6336 [==============================] - 1s 113us/step - loss: 0.4060 - acc: 0.8226 - val_loss: 0.4207 - val_acc: 0.8141\n",
      "845/845 [==============================] - 0s 35us/step\n",
      "Loss:  0.4331244908846342\n",
      "Accuracy :  81.06508877150405\n"
     ]
    }
   ],
   "source": [
    "#Using Trained Embedding GLoVe (on the sentences) for the Embedding Layer\n",
    "global sentence_list\n",
    "sentence_list[0].insert(0,0)\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentence_list, window=5)\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=10, no_threads=4, verbose=False)\n",
    "# Add the object to the dictionary\n",
    "glove.save('glove.model')\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "print(\"VOCAB SIZE: \",len(list(glove.dictionary.keys())))\n",
    "vocab_size = len(list(glove.dictionary.keys()))\n",
    "# print(glove.dictionary)\n",
    "\n",
    "# Glove Embedding Matrix\n",
    "embedding_matrix = createEmbMatrix(glove.word_vectors, glove.dictionary)\n",
    "print(\"Embedding Matrix : \",embedding_matrix.shape)\n",
    "\n",
    "print(\"================================ For SOCIAL ==============================\")\n",
    "# Split the data into train and validate\n",
    "sentences_train,sentences_val,y_train,y_val = train_test_split(\n",
    "                                                padded_sentence.tolist(), social_label,  \n",
    "                                                test_size=0.40,  \n",
    "                                                random_state=1000)\n",
    "sentences_validate, sentences_test, y_validate, y_test = train_test_split(\n",
    "                                                sentences_val, y_val,  \n",
    "                                                test_size=0.20)\n",
    "\n",
    "print(\"----------- TRAIN ---------------\")\n",
    "print(\"Shape: \",len(sentences_train))\n",
    "print(\"----------- VALIDATE --------------\")\n",
    "print(\"Shape: \",len(sentences_test))\n",
    "\n",
    "# CNN for Social\n",
    "runCNNModel(sentences_train, y_train, sentences_validate, y_validate, sentences_test, y_test)\n",
    "print(\"Using Glove trained on Model ===> \")\n",
    "gloveCNNModel(sentences_train, y_train,sentences_validate, y_validate, sentences_test, y_test, embedding_matrix)\n",
    "\n",
    "print(\"============================= For AGENCY =================================\")\n",
    "sentences_train,sentences_val,y_train,y_val = train_test_split(\n",
    "                                                padded_sentence.tolist(), agency_label,  \n",
    "                                                test_size=0.40,  \n",
    "                                                random_state=1000)\n",
    "sentences_validate, sentences_test, y_validate, y_test = train_test_split(\n",
    "                                                sentences_val, y_val,  \n",
    "                                                test_size=0.20)\n",
    "print(\"----------- TRAIN ---------------\")\n",
    "print(\"Shape: \",len(sentences_train))\n",
    "print(\"----------- VALIDATE --------------\")\n",
    "print(\"Shape: \",len(sentences_test))\n",
    "runCNNModel(sentences_train, y_train, sentences_validate, y_validate, sentences_test, y_test)\n",
    "print(\"Using Glove trained on Model ===> \")\n",
    "gloveCNNModel(sentences_train, y_train, sentences_validate, y_validate, sentences_test, y_test, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
