{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL5aA_1Sgkz5",
        "colab_type": "code",
        "outputId": "6f69d87f-37b3-4d84-fd5a-0a661bb34cd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!pip3 install glove-python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import operator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glove import Corpus, Glove\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: glove-python in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove-python) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove-python) (1.16.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7MvHhKegyPU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4HmITd9gk0u",
        "colab_type": "code",
        "outputId": "d2dee502-5d86-4271-d022-77af20d29b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# read the dataset \n",
        "data = pd.read_csv(\"dataset.csv\")\n",
        "print(data.shape)\n",
        "print(data.head())"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10560, 12)\n",
            "    hmid  ...                     duration\n",
            "0  27674  ...                   half_a_day\n",
            "1  27685  ...                   half_a_day\n",
            "2  27691  ...            at_least_one_hour\n",
            "3  27701  ...  all_day_im_still_feeling_it\n",
            "4  27712  ...                 a_few_moment\n",
            "\n",
            "[5 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYZk3lk-gk03",
        "colab_type": "code",
        "outputId": "cb6abbd7-aace-4ce3-dcd5-36324cf8386c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Get Moment, agency and social\n",
        "target_data = data[['moment', 'agency', 'social']]\n",
        "print(target_data)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  moment agency social\n",
            "0      I was happy when my son got 90% marks in his e...     no    yes\n",
            "1              went to movies with my friends it was fun    yes    yes\n",
            "2      A hot kiss with my girl friend last night made...    yes    yes\n",
            "3      My son woke me up to a fantastic breakfast of ...     no    yes\n",
            "4      My older daughter keeps patting my younger dau...     no    yes\n",
            "5          I cooked my girlfriend a wonderful breakfast.    yes    yes\n",
            "6         My Mother gave me a surprise visit at my home.     no    yes\n",
            "7      There was hardly any traffic on my way to work...    yes     no\n",
            "8                     I came to my office at right time.    yes     no\n",
            "9      The day I got my degree in industrial engineering    yes     no\n",
            "10     I went to office hour of one of my professors,...    yes    yes\n",
            "11     We all ladies member from my family went for a...    yes    yes\n",
            "12     When my wife came home from work and we shared...    yes    yes\n",
            "13                        My father bought me a bicycle.     no    yes\n",
            "14     I went to my home and given an Ice Cream Famil...    yes    yes\n",
            "15     I was able to play my video game that I enjoy ...    yes     no\n",
            "16     I was able to do a full hour of research on a ...    yes     no\n",
            "17     When i got to the train station it just pulled...     no     no\n",
            "18     I went to support my cousin in her product lau...    yes    yes\n",
            "19     I cooked a new recipe for dinner and it turned...    yes     no\n",
            "20     The weather has been warm and gorgeous for the...     no     no\n",
            "21     Had dinner with my Family, now that my dad is ...    yes    yes\n",
            "22     A hot chat with my dating partner makes me fee...    yes    yes\n",
            "23     The rain stopped and beautiful sunlight appear...    yes     no\n",
            "24                          I had a very delicious lunch    yes     no\n",
            "25     I finally went for a run and worked out and it...    yes     no\n",
            "26     I received a call letter from the University f...     no     no\n",
            "27                     my hole family out to have dinner    yes    yes\n",
            "28     yesterday i received a message that i got prom...    yes    yes\n",
            "29                         A good win for my sports team     no    yes\n",
            "...                                                  ...    ...    ...\n",
            "10530                                 I WENT TO SHOPPING    yes     no\n",
            "10531  My uncle is working abroad, when he is here, I...     no    yes\n",
            "10532  Enjoying my new pair of sunnies, riding around...    yes     no\n",
            "10533  My uncle is working abroad, when he is here, I...     no    yes\n",
            "10534  MY UNCLE IS WORKING IN  ABROAD WHEN HE IS HERE...     no    yes\n",
            "10535  We buyed some cookies, sweets, clothes to our ...    yes    yes\n",
            "10536                farewell party to enjoy my friends.    yes    yes\n",
            "10537          My cousin made it through surgery safely.     no    yes\n",
            "10538  A new episode of my favorite podcasts came out...     no     no\n",
            "10539  One event that made me feel happy last night w...    yes    yes\n",
            "10540                 i went to my old school yesterday.    yes     no\n",
            "10541          I made big progress on a project at work.    yes     no\n",
            "10542     Spent all almost all day with my granddaughter    yes    yes\n",
            "10543             I enjoyed a home-cooked Southern meal.    yes     no\n",
            "10544  Listening to my favourite music in my car on t...    yes     no\n",
            "10545    I ate some delicious eggrolls that my mom made.    yes     no\n",
            "10546  I'm reading the 3rd novel of Jacqueline Carey'...    yes     no\n",
            "10547  I had a very nice meal with my spouse without ...    yes    yes\n",
            "10548  I read a new novel which was so interesting an...    yes     no\n",
            "10549  We all family members went to a nearby restaur...    yes    yes\n",
            "10550  I was praised by my boss for an idea I had at ...     no    yes\n",
            "10551  A happy event that made me happy in the past 2...    yes     no\n",
            "10552  I spent time with my wife watching TV last night.    yes    yes\n",
            "10553  I accepted financial aid for this upcoming sch...    yes     no\n",
            "10554  Yesterday after court I took an extra hour to ...    yes    yes\n",
            "10555  My husband called me just to tell me he loved me.     no    yes\n",
            "10556     I worked out, which always makes me feel good.    yes     no\n",
            "10557  Finally got to watch the new Resident Evil movie.    yes     no\n",
            "10558  I got to talk to an old friend and reminisce o...    yes    yes\n",
            "10559  I had a great meeting yesterday at work with m...    yes    yes\n",
            "\n",
            "[10560 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FVaV6lvgk0_",
        "colab_type": "code",
        "outputId": "cc037aae-9f2a-470f-ac39-317451153fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Data Analysis\n",
        "global agency\n",
        "global social\n",
        "agency = list(target_data['agency'])\n",
        "moments = list(target_data['moment'])\n",
        "social = list(target_data['social'])\n",
        "yy = 0\n",
        "yn = 0\n",
        "nn = 0\n",
        "ny = 0\n",
        "# print(\"Len: \",len(moments))\n",
        "for i in range(len(moments)):\n",
        "    if agency[i] == 'yes' and social[i] =='yes':\n",
        "        yy += 1\n",
        "    elif agency[i] == 'yes' and social[i] == 'no':\n",
        "        yn += 1\n",
        "    elif agency[i] == 'no' and social[i] =='yes':\n",
        "        ny += 1\n",
        "    else:\n",
        "        nn += 1\n",
        "\n",
        "print(\"------------------ Social -------------------\")\n",
        "print(\"               YES        NO      SUM\")\n",
        "print(\"Agency YES    \",yy,\"     \",yn,\"  \",yy+yn)\n",
        "print(\"       NO     \",ny,\"     \",nn,\"   \",ny+nn)\n",
        "print(\"       SUM    \",yy+ny,\"     \",yn+nn,\"  \",yy+nn+ny+yn)\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------ Social -------------------\n",
            "               YES        NO      SUM\n",
            "Agency YES     3554       4242    7796\n",
            "       NO      2071       693     2764\n",
            "       SUM     5625       4935    10560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp4IanWJgk1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Observation: Data is positive in high proportion although there is an imbalance as more positive data\n",
        "# for social than for agency. This might result in poor accuracy while predicting the label agency."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGYZbWNmgk1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "import statistics\n",
        "def tokenizer(sentence):\n",
        "    tokenizer = RegexpTokenizer('[a-zA-Z0-9\\']+')\n",
        "    words = tokenizer.tokenize(sentence)\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqhdcP3Rgk1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "def stopWordRemoval(words):\n",
        "    filtered_data = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    for word in words:\n",
        "        if not word in stop_words:\n",
        "            if word.isnumeric() or word.isalpha():\n",
        "                filtered_data.append(word)\n",
        "    return filtered_data\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP1cFo3Ogk1b",
        "colab_type": "code",
        "outputId": "9fea4423-3839-4fc6-e7bd-0d6ea482562b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Data Pre-processing\n",
        "global sentence_list\n",
        "global sequenced_vocab\n",
        "global sentences\n",
        "global vocab\n",
        "# 1. Split the sentences in moments into words (using regex tokenizer)\n",
        "vocab = list()\n",
        "sequenced_vocab = dict()\n",
        "index = 1\n",
        "sentence_list = list()\n",
        "seq_list = list()\n",
        "sentences = list()\n",
        "\n",
        "for i in range(len(moments)):\n",
        "    moments[i] = moments[i].lower()\n",
        "    bag_of_words = tokenizer(moments[i])\n",
        "    #remove stop words\n",
        "#     bag_of_words = stopWordRemoval(bag_of_words)\n",
        "    sentence_list.append(bag_of_words)\n",
        "    vocab += bag_of_words\n",
        "\n",
        "# sentences = sentence_list\n",
        "\n",
        "# Form sequence dictionary and convert each sentence into a number sequence \n",
        "for word in vocab:\n",
        "    if word not in sequenced_vocab.keys():\n",
        "        sequenced_vocab[word] = index\n",
        "        index += 1\n",
        "print(\"Unique words in Vocabulary : \",len(sequenced_vocab))\n",
        "# print(\"Sequenced Vocab : \")\n",
        "# print(sequenced_vocab)\n",
        "\n",
        "# Avg length of each sentence\n",
        "sentence_length = [ len(sentence_list[i]) for i in range(len(sentence_list))]\n",
        "sentence_length.sort()\n",
        "print(\"------------------ SENTENCE STATISTICS OF MOMENTS -----------------\")\n",
        "print(\"Minimum Length : \",sentence_length[0])\n",
        "print(\"Maximum Length : \",sentence_length[len(sentence_length)-1])\n",
        "print(\"Average Length : \",round(sum(sentence_length)/len(sentence_length)))\n",
        "print(\"Median Length  : \", round(statistics.median(sentence_length)))\n",
        "# print(sentence_list)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in Vocabulary :  7604\n",
            "------------------ SENTENCE STATISTICS OF MOMENTS -----------------\n",
            "Minimum Length :  2\n",
            "Maximum Length :  70\n",
            "Average Length :  13\n",
            "Median Length  :  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdHRDphfvyic",
        "colab_type": "code",
        "outputId": "64efcee2-d8aa-479c-da45-3dd9d666d5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# 2. Assign a sequence of numbers to each of the sentences \n",
        "global sequenced_vocab\n",
        "for i in range(len(sentence_list)):\n",
        "    for word in sentence_list[i]:\n",
        "        if word in sequenced_vocab.keys():\n",
        "            seq_list.append(sequenced_vocab[word])\n",
        "    sentence_list[i] = seq_list\n",
        "    seq_list = []\n",
        "    \n",
        "# print(\" SEQUENCED SENTENCES  \")\n",
        "# print(sentence_list)\n",
        "\n",
        "#Printing Sentence Stats\n",
        "count_len = dict()\n",
        "for length in sentence_length:\n",
        "    if length not in count_len.keys():\n",
        "        count_len[length] = 1\n",
        "    else:\n",
        "        count_len[length] += 1\n",
        "\n",
        "print(\"Total Sentences : \",len(sentence_list))\n",
        "print(\"Length of majority sentences: \",max(count_len.items(), key=operator.itemgetter(1))[0])\n",
        "    \n",
        "# word_list = list(sequenced_vocab.keys())\n",
        "# indexes = list(sequenced_vocab.values())\n",
        "# print(\"Indexs: \",indexes)\n",
        "\n",
        "# # 3. Pad with 0s if sentence length is not same (at the beginning)\n",
        "padded_sentence = pad_sequences(sentence_list, maxlen=20)\n",
        "print(\" ================= PADDED SENTENCE =================\")\n",
        "print(padded_sentence[500])"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sentences :  10560\n",
            "Length of majority sentences:  8\n",
            " ================= PADDED SENTENCE =================\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    1\n",
            "  363 1431   16   20  616   23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjlJ_N4Dgk1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transform agency and social to binary labels\n",
        "global agency_label\n",
        "global social_label\n",
        "agency_label = [ 0 if agency[i] == \"no\" else 1 for i in range(len(agency))]\n",
        "social_label = [ 0 if social[i] == \"no\" else 1 for i in range(len(social))]\n",
        "# print(agency_label)\n",
        "# print(social_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhjTDETjRqnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createEmbMatrix(vectors, word_dict):\n",
        "  max_words = len(list(word_dict.keys()))\n",
        "  embedding_matrix = np.zeros((max_words, 100))\n",
        "  for i in range(max_words):\n",
        "    embedding_vector = vectors[i]\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0G_5A82rmRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def runCNNModel(sentences_train, y_train, sentences_test, y_test):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(7920,100,input_length=20))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Conv1D(64,3,padding='valid',activation='relu',strides=1))\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(layers.Dense(256))\n",
        "  # model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.add(layers.Activation('sigmoid'))\n",
        "  model.summary()\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(np.array(sentences_train), np.array(y_train),\n",
        "                      epochs=10,\n",
        "                      batch_size=32,\n",
        "                      verbose=1,\n",
        "                      validation_data=(np.array(sentences_test), np.array(y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP3V4jAggk1n",
        "colab_type": "code",
        "outputId": "ad920272-43a9-463b-f506-1a0d11f21f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Using Pre-Trained Embedding GLoVe for the Embedding Layer\n",
        "# corpus = Corpus()\n",
        "# # print(sentences)\n",
        "# corpus.fit(padded_sentence.tolist(), window=5)\n",
        "# glove = Glove(no_components=100, learning_rate=0.05)\n",
        "# glove.fit(corpus.matrix, epochs=10, no_threads=4, verbose=True)\n",
        "# # Add the object to the dictionary\n",
        "# glove.save('glove.model')\n",
        "# glove.add_dictionary(corpus.dictionary)\n",
        "# print(glove.word_vectors.shape)\n",
        "# print(\"VOCAB SIZE: \",len(list(glove.dictionary.keys())))\n",
        "# vocab_size = len(list(glove.dictionary.keys()))\n",
        "\n",
        "# ----------------------------------------------- Without GloVe Embedding Matrix -------------------------------------------\n",
        "# Split the data into train and validate\n",
        "sentences_train,sentences_test,y_train,y_test = train_test_split(\n",
        "                                                padded_sentence.tolist(), social_label,  \n",
        "                                                test_size=0.25,  \n",
        "                                                random_state=1000)\n",
        "print(\"----------- TRAIN ---------------\")\n",
        "print(\"Shape: \",len(sentences_train))\n",
        "print(\"----------- VALIDATE --------------\")\n",
        "print(\"Shape: \",len(sentences_test))\n",
        "\n",
        "# Glove Embedding Matrix\n",
        "embedding_matrix = createEmbMatrix(glove.word_vectors, glove.dictionary)\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "# CNN for Social and Agency\n",
        "print(\"================================ For SOCIAL ==============================\")\n",
        "runCNNModel(sentences_train, y_train, sentences_test, y_test)\n",
        "\n",
        "print(\"============================= For AGENCY =================================\")\n",
        "sentences_train,sentences_test,y_train,y_test = train_test_split(\n",
        "                                                padded_sentence.tolist(), agency_label,  \n",
        "                                                test_size=0.25,  \n",
        "                                                random_state=1000)\n",
        "print(\"----------- TRAIN ---------------\")\n",
        "print(\"Shape: \",len(sentences_train))\n",
        "print(\"----------- VALIDATE --------------\")\n",
        "print(\"Shape: \",len(sentences_test))\n",
        "runCNNModel(sentences_train, y_train, sentences_test, y_test)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------- TRAIN ---------------\n",
            "Shape:  7920\n",
            "----------- VALIDATE --------------\n",
            "Shape:  2640\n",
            "(7347, 100)\n",
            "================================ For SOCIAL ==============================\n",
            "Model: \"sequential_37\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_31 (Embedding)     (None, 20, 100)           792000    \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_30 (Conv1D)           (None, 18, 64)            19264     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_30 (Glo (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 828,161\n",
            "Trainable params: 828,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 7920 samples, validate on 2640 samples\n",
            "Epoch 1/10\n",
            "7920/7920 [==============================] - 6s 721us/step - loss: 0.3813 - acc: 0.8246 - val_loss: 0.2612 - val_acc: 0.8996\n",
            "Epoch 2/10\n",
            "7920/7920 [==============================] - 4s 466us/step - loss: 0.2001 - acc: 0.9287 - val_loss: 0.2553 - val_acc: 0.9042\n",
            "Epoch 3/10\n",
            "7920/7920 [==============================] - 4s 465us/step - loss: 0.1267 - acc: 0.9577 - val_loss: 0.2884 - val_acc: 0.8973\n",
            "Epoch 4/10\n",
            "7920/7920 [==============================] - 4s 471us/step - loss: 0.0793 - acc: 0.9744 - val_loss: 0.3443 - val_acc: 0.8879\n",
            "Epoch 5/10\n",
            "7920/7920 [==============================] - 4s 486us/step - loss: 0.0466 - acc: 0.9865 - val_loss: 0.4083 - val_acc: 0.8803\n",
            "Epoch 6/10\n",
            "7920/7920 [==============================] - 4s 474us/step - loss: 0.0299 - acc: 0.9917 - val_loss: 0.4653 - val_acc: 0.8807\n",
            "Epoch 7/10\n",
            "7920/7920 [==============================] - 4s 479us/step - loss: 0.0214 - acc: 0.9941 - val_loss: 0.5355 - val_acc: 0.8777\n",
            "Epoch 8/10\n",
            "7920/7920 [==============================] - 4s 483us/step - loss: 0.0155 - acc: 0.9957 - val_loss: 0.5529 - val_acc: 0.8720\n",
            "Epoch 9/10\n",
            "7920/7920 [==============================] - 4s 479us/step - loss: 0.0119 - acc: 0.9966 - val_loss: 0.5895 - val_acc: 0.8739\n",
            "Epoch 10/10\n",
            "7920/7920 [==============================] - 4s 471us/step - loss: 0.0088 - acc: 0.9977 - val_loss: 0.6236 - val_acc: 0.8723\n",
            "============================= For AGENCY =================================\n",
            "----------- TRAIN ---------------\n",
            "Shape:  7920\n",
            "----------- VALIDATE --------------\n",
            "Shape:  2640\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_32 (Embedding)     (None, 20, 100)           792000    \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 20, 100)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_31 (Conv1D)           (None, 18, 64)            19264     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_31 (Glo (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 828,161\n",
            "Trainable params: 828,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 7920 samples, validate on 2640 samples\n",
            "Epoch 1/10\n",
            "7920/7920 [==============================] - 6s 752us/step - loss: 0.4339 - acc: 0.7970 - val_loss: 0.3596 - val_acc: 0.8367\n",
            "Epoch 2/10\n",
            "7920/7920 [==============================] - 4s 484us/step - loss: 0.2944 - acc: 0.8777 - val_loss: 0.3646 - val_acc: 0.8379\n",
            "Epoch 3/10\n",
            "7920/7920 [==============================] - 4s 493us/step - loss: 0.1947 - acc: 0.9253 - val_loss: 0.4049 - val_acc: 0.8337\n",
            "Epoch 4/10\n",
            "7920/7920 [==============================] - 4s 495us/step - loss: 0.1137 - acc: 0.9609 - val_loss: 0.4803 - val_acc: 0.8265\n",
            "Epoch 5/10\n",
            "7920/7920 [==============================] - 4s 488us/step - loss: 0.0639 - acc: 0.9801 - val_loss: 0.6002 - val_acc: 0.8186\n",
            "Epoch 6/10\n",
            "7920/7920 [==============================] - 4s 484us/step - loss: 0.0394 - acc: 0.9879 - val_loss: 0.6199 - val_acc: 0.8117\n",
            "Epoch 7/10\n",
            "7920/7920 [==============================] - 4s 487us/step - loss: 0.0258 - acc: 0.9932 - val_loss: 0.7161 - val_acc: 0.8174\n",
            "Epoch 8/10\n",
            "7920/7920 [==============================] - 4s 481us/step - loss: 0.0214 - acc: 0.9946 - val_loss: 0.7739 - val_acc: 0.8121\n",
            "Epoch 9/10\n",
            "7920/7920 [==============================] - 4s 474us/step - loss: 0.0140 - acc: 0.9966 - val_loss: 0.8664 - val_acc: 0.8155\n",
            "Epoch 10/10\n",
            "7920/7920 [==============================] - 4s 477us/step - loss: 0.0151 - acc: 0.9971 - val_loss: 0.8341 - val_acc: 0.8076\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}